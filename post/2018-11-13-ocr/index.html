<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A technical view of FVI: end-to-end Vietnamese ID card OCR | hiepph</title><meta name=keywords content><meta name=description content="I’m currently working on an OCR project with some of my Vision researcher/engineer colleagues: FVI. The job is to extract pieces of information from a Vietnamese ID card.
In research progress, I wandered the internet and found some useful articles (e.g. Dropbox, Zocdoc, Facebook) about how to build an OCR system. But none of this explained clearly to me a complete intuition how to bring these research models into a production environment."><meta name=author content><link rel=canonical href=https://hiepph.xyz/post/2018-11-13-ocr/><link crossorigin=anonymous href=/assets/css/stylesheet.min.2d6dbfc6e0f8a1db1c9d082a76dc11d094328cf63f247bbc2421dfaa7f2bb170.css integrity="sha256-LW2/xuD4odscnQgqdtwR0JQyjPY/JHu8JCHfqn8rsXA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://hiepph.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://hiepph.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hiepph.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://hiepph.xyz/apple-touch-icon.png><link rel=mask-icon href=https://hiepph.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.83.1"><meta property="og:title" content="A technical view of FVI: end-to-end Vietnamese ID card OCR"><meta property="og:description" content="I’m currently working on an OCR project with some of my Vision researcher/engineer colleagues: FVI. The job is to extract pieces of information from a Vietnamese ID card.
In research progress, I wandered the internet and found some useful articles (e.g. Dropbox, Zocdoc, Facebook) about how to build an OCR system. But none of this explained clearly to me a complete intuition how to bring these research models into a production environment."><meta property="og:type" content="article"><meta property="og:url" content="https://hiepph.xyz/post/2018-11-13-ocr/"><meta property="article:section" content="post"><meta property="article:published_time" content="2018-11-13T00:00:00+00:00"><meta property="article:modified_time" content="2018-11-13T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A technical view of FVI: end-to-end Vietnamese ID card OCR"><meta name=twitter:description content="I’m currently working on an OCR project with some of my Vision researcher/engineer colleagues: FVI. The job is to extract pieces of information from a Vietnamese ID card.
In research progress, I wandered the internet and found some useful articles (e.g. Dropbox, Zocdoc, Facebook) about how to build an OCR system. But none of this explained clearly to me a complete intuition how to bring these research models into a production environment."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://hiepph.xyz/post/"},{"@type":"ListItem","position":3,"name":"A technical view of FVI: end-to-end Vietnamese ID card OCR","item":"https://hiepph.xyz/post/2018-11-13-ocr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A technical view of FVI: end-to-end Vietnamese ID card OCR","name":"A technical view of FVI: end-to-end Vietnamese ID card OCR","description":"I’m currently working on an OCR project with some of my Vision researcher/engineer colleagues: FVI. The job is to extract pieces of information from a Vietnamese ID card.\nIn research progress, I wandered the internet and found some useful articles (e.g. Dropbox, Zocdoc, Facebook) about how to build an OCR system. But none of this explained clearly to me a complete intuition how to bring these research models into a production environment.","keywords":[],"articleBody":"I’m currently working on an OCR project with some of my Vision researcher/engineer colleagues: FVI. The job is to extract pieces of information from a Vietnamese ID card.\nIn research progress, I wandered the internet and found some useful articles (e.g. Dropbox, Zocdoc, Facebook) about how to build an OCR system. But none of this explained clearly to me a complete intuition how to bring these research models into a production environment. So our team had a hard time (roughly 6 months) struggling to build an accurate, production-ready and scalable OCR system.\nAnd in this post, I want to share with you our experience (research, architecture, and deploy process) in hope to clear the mist of building and deploying a complete Deep Learning project in general, and OCR task in particular.\nStructure Our structure consists of 3 basic components: Cropper, Detector, Reader. Each component has its own model and train/validation/test process. So it can be easy to plug and play plugins to improve the system, better than a black box of a single end-to-end model.\n1. Cropper This component locates 4 corners of the ID card and then crop its quadrangular shape. The meaning of this is to make easier for word detection tasks (e.g. reduce noises and variances) which comes after.\nSince the common object detection model only returns 2 corners (a rectangular box), we use a little trick by treating each corner as an object with its own unique class and then detect 4 corners. The geometric transformation after locating 4 corners is trivial.\nOur detection model we are using is single shot detector: SSD (SSD: Single Shot MultiBox Detector), with feature extractor is MobileNet v2 (MobileNetV2: Inverted Residuals and Linear Bottlenecks).\nSSD provides us fast inference speed, while MobileNet v2 decreases the number of operations and memory but still preserves good accuracy.\nImage courtesy of Matthijs Hollemans, source\n2. Detector This component extracts rectangular shapes contain word tokens belongs to each class (e.g. ID number, name, address, date of birth). We sort them depends on coordinates and then parse them to the Reader.\nWe also utilizes SSD for this task just like the Cropper, but with different feature extractor: Resnet FPN (Deep Residual Learning for Image Recognition, Feature Pyramid Networks for Object Detection)\nImage in the FPN paper, and courtesy of Jonathan Hui, source\nResnet FPN assures us state-of-the-art accuracy, and supports multi-scale of an image so the model can deal with various input situations.\n3. Reader Given some words and their orders for each region class, we do batch inferencing to the Reader model to get the string results.\nThe model architecture we use is a word-level reader, utilizes the Google’s Attention OCR architecture ( Attention-based Extraction of Structured Information from Street View Imagery) with some little tweaks.\nImage in Attention OCR paper\nFirst, we use 1 view instead of 4 views for each word image because the text detected is mostly vertical after the Cropper phase. And then use the same Inception Resnet v2 layer (Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning) cut at Mixed_6a for feature extraction, followed by LSTM layer and then attention decoder.\nFor gradient descent optimization, we use Adadelta with initial learning rate 1.0 rather than stochastic gradient descent (SGD) with momentum described in the paper. Mainly reason is Adadelta adapts the learning rate to the parameters, thus no troubling tuning the learning rate in train process.\nThis Reader model achieves more than 99% for character accuracy and often with 1-2% decreases in word accuracy. The combined end-to-end system (Cropper - Detector - Reader) achieves approximately 90% accuracy for each region class.\nTheoretically, we can do better with some strategy like synthetic data, curriculum training, and so much more. But with the first public version we think this is enough and decide to give it a go in the production, and then update follow client feedbacks.\nArchitecture Here a diagram of our structure used in a real-life scenario.\nThe problem is how to bring these components into the production environment.\nThe naive way is to pre-load the trained checkpoints and write some additional query function to infer the model. But it’s not an optimized way. It eats resources with bloated memory, has high latency queries and high risk of CPU spikes.\nA better way is to freeze to model first (called a Frozen Graph, tutorial like this) to clear the mentioned problem.\nBut we want a more mature way of serving the trained models, say low-latency high-throughput requests, zero-downtime model update, batch inferencing request, consistent API for inferencing. Luckily, Google has a solution for us, enter Tensorflow Serving.\nTensorflow Serving Tensorflow Serving uses SavedModel with version tag. For example, here I have a 1-version Cropper, 1-version Detector, and 2-version Reader.\nCropper/ 1/ Detector/ 1/ Reader/ 1/ 2/ saved_model.pb variables/ variables.index And you can load all of these models into Tensorflow Serving at once (YES, multiple models serving) by specifying a model base path config (for example, serving.config):\nmodel_config_list: { config: { name: \"cropper\", base_path: \"/absolute/path/to/cropper\", model_platform: \"tensorflow\" }, config: { name: \"detector\", base_path: \"/absolute/path/to/detector\", model_platform: \"tensorflow\" }, config: { name: \"reader\", base_path: \"/absolute/path/to/reader\", model_platform: \"tensorflow\" } } and boot up Tensorflow Serving with appropriate flag:\ntensorflow_model_server --model_config_file=/absolute/path/to/serving.config The trouble way is you have to export model with trained weights to SavedModel format first.\nA sample script to export I provide here. The trick is you make use of Tensorflow Saver.\n Specifically, SavedModel wraps a TensorFlow Saver. The Saver is primarily used to generate the variable checkpoints. source\n And the second important thing is you understand what input and output node your model is having.\n# ... with tf.Session() as sess: tf.train.Saver().restore(sess, FLAGS.checkpoint) # images_placeholder as input node inputs = {'input': tf.saved_model.utils.build_tensor_info(INPUT_PLACEHOLDER)} # get output node through node name out_classes = sess.graph.get_tensor_by_name('YOUR_OUTPUT_NODE_NAME') outputs = {'output': tf.saved_model.utils.build_tensor_info(out_classes)} signature = tf.saved_model.signature_def_utils.build_signature_def( inputs=inputs, outputs=outputs, method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME) legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op') # Save out the SavedModel builder = tf.saved_model.builder.SavedModelBuilder(FLAGS.saved_dir) builder.add_meta_graph_and_variables( sess, [tf.saved_model.tag_constants.SERVING], signature_def_map={ tf.saved_model.signature_constants. DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature }, legacy_init_op=legacy_init_op) builder.save() Backend service Now is the full look of our backend service:\nThere are 2 small services (components) we mainly take care of: api and serving.\nserving is Tensorflow Serving as we mentioned, and api (which provides RESTful API for clients) connects with serving through grpc. Luckily (also), we don’t have to take care much of Protocol Buffers logics and just make use of tensorflow-serving-api library. A sample use I provide:\nimport tensorflow as tf from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis.prediction_service_pb2_grpc import PredictionServiceStub import grpc import cv2 # Your config SERVICE_HOST = 'localhost:8500' # sample MODEL_NAME = 'reader' INPUT_SIGNATURE_KEY = 'input' OUTPUT_SIGNATURE_KEY = 'output' ####### # Connect to server channel = grpc.insecure_channel(SERVICE_HOST) stub = PredictionServiceStub(channel) # Prepare request object request = predict_pb2.PredictRequest() request.model_spec.name = MODEL_NAME request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY # Copy image into request's content img = cv2.imread('/path/to/image') input_tensor = np.expand_dims(img, axis=0) # we do batch inferencing, so input is a 4-D tensor request.inputs[INPUT_SIGNATURE_KEY].CopyFrom( tf.contrib.util.make_tensor_proto(input_tensor)) # Do inference result = stub.Predict.future(request, 5) # 5s timeout # Get output depends on our input/output signature, and their types # for example, our output signature key is 'output' and has string value word = result.result().outputs[OUTPUT_SIGNATURE_KEY].string_val[0] # we have batch result, so just take first index Deployment Deployment is an important part, but usually a myth among Deep Learning articles. Tensorflow Serving guide demos deploy in Kubernetes with built container images. But we found it over-complicated and decided to use Docker Compose for simply running two pre-built images api and serving.\nWe have two types of deploy machines, only CPU platform, and with GPU platform.\nOnly CPU machine goes with common docker-compose.yml, with Dockerfile for api, and Dockerfile.serving for serving which is based from tensorflow/serving:latest-devel image.\nOn the other hand, GPU machine goes with custom nvidia-docker-compose.yml (which requires nvidia-docker), same Dockerfile for api, and Dockerfile.serving.gpu for serving (which is based from tensorflow:latest-devel-gpu).\nOne main note for running Docker Compose with GPU is inside nvidia-docker-compose.yml you have to specify runtime: nvidia. A troublesome problem is Docker Compose default uses all GPUs so you have to set NVIDIA_VISIBLE_DEVICES environment variable to your dedicated running GPUs. A trick I use for consistent use between Tensorflow (CUDA_VISIBLE_DEVICES) and Docker Compose is:\n# nvidia-docker-compose.yml version: '2.3' services: serving: runtime: nvidia environment: NVIDIA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-all} [...] [...] By doing that, if you set CUDA_VISIBLE_DEVICES you use what GPUs you want to use. Otherwise it uses all GPUs by default.\nSummary I shared you our research, architecture, and deployment of a complete OCR system. It uses the state-of-the-art deep learning OCR model (Attention OCR), scalable with Tensorflow Serving, and ready for production deployment with the help of Docker Compose.\nBy using Tensorflow we have an entire ecosystem backed by Google, a typical benefit is Tensorflow Serving (which belongs to TFX). A huge support is from the community for model implementation, too.\n","wordCount":"1462","inLanguage":"en","datePublished":"2018-11-13T00:00:00Z","dateModified":"2018-11-13T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://hiepph.xyz/post/2018-11-13-ocr/"},"publisher":{"@type":"Organization","name":"hiepph","logo":{"@type":"ImageObject","url":"https://hiepph.xyz/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://hiepph.xyz accesskey=h title="hiepph (Alt + H)">hiepph</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A technical view of FVI: end-to-end Vietnamese ID card OCR</h1><div class=post-meta>November 13, 2018</div></header><div class=post-content><p>I’m currently working on an <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a> project with some of my Vision researcher/engineer colleagues: <strong>FVI</strong>.
The job is to extract pieces of information from a Vietnamese ID card.</p><p>In research progress, I wandered the internet and found some useful articles
(e.g. <a href=https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/>Dropbox</a>,
<a href=https://www.zocdoc.com/about/blog/tech/making-sense-of-insurance-cards-using-deep-learning/>Zocdoc</a>,
<a href=https://code.fb.com/ai-research/rosetta-understanding-text-in-images-and-videos-with-machine-learning/>Facebook</a>)
about how to build an OCR system.
But none of this explained clearly to me a complete intuition how to bring these research models into a production environment.
So our team had a hard time (roughly 6 months) struggling to build an accurate, production-ready and scalable OCR system.</p><p>And in this post, I want to share with you our experience (research, architecture, and deploy process) in hope to
clear the mist of building and deploying a complete Deep Learning project in general, and OCR task in particular.</p><h1 id=structure>Structure<a hidden class=anchor aria-hidden=true href=#structure>#</a></h1><p>Our structure consists of 3 basic components: Cropper, Detector, Reader.
Each component has its own model and train/validation/test process.
So it can be easy to plug and play plugins to improve the system, better than a black box of a single end-to-end model.</p><p><img loading=lazy src=/ocr/Structure.png alt=structure></p><h2 id=1-cropper>1. Cropper<a hidden class=anchor aria-hidden=true href=#1-cropper>#</a></h2><p>This component locates 4 corners of the ID card and then crop its quadrangular shape.
The meaning of this is to make easier for word detection tasks (e.g. reduce noises and variances) which comes after.</p><p>Since the common object detection model only returns 2 corners (a rectangular box),
we use a little trick by treating each corner as an object with its own unique class and then detect 4 corners.
The geometric transformation after locating 4 corners is trivial.</p><p><img loading=lazy src=/ocr/cropper.png alt=cropper></p><p>Our detection model we are using is single shot detector: SSD (<a href=https://arxiv.org/abs/1512.02325>
SSD: Single Shot MultiBox Detector</a>), with feature extractor is MobileNet v2
(<a href=https://arxiv.org/abs/1801.04381>MobileNetV2: Inverted Residuals and Linear Bottlenecks</a>).</p><p>SSD provides us fast inference speed, while MobileNet v2 decreases the number of operations and memory but still preserves good accuracy.</p><p><img loading=lazy src=http://machinethink.net/images/mobilenet-v2/FeatureExtractor.png alt=ssd-mobilenet></p><p><em>Image courtesy of Matthijs Hollemans, <a href=http://machinethink.net/blog/mobilenet-v2/>source</a></em></p><h2 id=2-detector>2. Detector<a hidden class=anchor aria-hidden=true href=#2-detector>#</a></h2><p>This component extracts rectangular shapes contain word tokens belongs to each class (e.g. ID number, name, address, date of birth).
We sort them depends on coordinates and then parse them to the Reader.</p><p><img loading=lazy src=/ocr/detector.png alt=detector></p><p>We also utilizes SSD for this task just like the Cropper, but with different feature extractor: Resnet FPN
(<a href=https://arxiv.org/abs/1512.03385>Deep Residual Learning for Image Recognition</a>,
<a href=https://arxiv.org/abs/1612.03144>Feature Pyramid Networks for Object Detection</a>)</p><p><img loading=lazy src=https://cdn-images-1.medium.com/max/800/1*aMRoAN7CtD1gdzTaZIT5gA.png alt=fpn></p><p><em>Image in the FPN <a href=https://arxiv.org/abs/1612.03144>paper</a>, and courtesy of Jonathan Hui, <a href=https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c>source</a></em></p><p>Resnet FPN assures us state-of-the-art accuracy,
and supports multi-scale of an image so the model can deal with various input situations.</p><h2 id=3-reader>3. Reader<a hidden class=anchor aria-hidden=true href=#3-reader>#</a></h2><p>Given some words and their orders for each region class, we do batch inferencing to the Reader model
to get the string results.</p><p>The model architecture we use is a word-level reader, utilizes the Google&rsquo;s Attention OCR architecture (
<a href=https://arxiv.org/abs/1704.03549>Attention-based Extraction of Structured Information from Street View Imagery</a>) with some little tweaks.</p><p><img loading=lazy src=/ocr/attention.png alt=reader></p><p><em>Image in Attention OCR <a href=https://arxiv.org/abs/1704.03549>paper</a></em></p><p>First, we use 1 view instead of 4 views for each word image because the text detected is mostly vertical after the Cropper phase.
And then use the same Inception Resnet v2 layer
(<a href=https://arxiv.org/abs/1602.07261>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a>) cut at <code>Mixed_6a</code> for feature extraction,
followed by LSTM layer and then attention decoder.</p><p>For gradient descent optimization, we use <a href=http://ruder.io/optimizing-gradient-descent/index.html#adadelta>Adadelta</a>
with initial learning rate 1.0 rather than stochastic gradient descent (SGD) with momentum described in the paper.
Mainly reason is Adadelta adapts the learning rate to the parameters, thus no troubling tuning the learning rate in train process.</p><p>This Reader model achieves more than 99% for character accuracy and often with 1-2% decreases in word accuracy.
The combined end-to-end system (Cropper -> Detector -> Reader) achieves approximately 90% accuracy for each region class.</p><p>Theoretically, we can do better with some strategy like synthetic data, curriculum training, and so much more. But with the first
public version we think this is enough and decide to give it a go in the production, and then update follow client feedbacks.</p><h1 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h1><p>Here a diagram of our structure used in a real-life scenario.</p><p><img loading=lazy src=/ocr/architecture.png alt=architecture></p><p>The problem is how to bring these components into the production environment.</p><p>The naive way is to pre-load the trained checkpoints and write some additional query function to infer the model.
But it&rsquo;s not an optimized way. It eats resources with bloated memory, has high latency queries and high risk of CPU spikes.</p><p>A better way is to freeze to model first (called a Frozen Graph, tutorial like <a href=https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc>this</a>)
to clear the mentioned problem.</p><p>But we want a more mature way of serving the trained models, say low-latency high-throughput requests, zero-downtime model update, batch inferencing request, consistent API for inferencing.
Luckily, Google has a solution for us, enter <a href=https://www.tensorflow.org/serving/>Tensorflow Serving</a>.</p><h2 id=tensorflow-serving>Tensorflow Serving<a hidden class=anchor aria-hidden=true href=#tensorflow-serving>#</a></h2><p>Tensorflow Serving uses <a href=https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model>SavedModel</a> with version tag.
For example, here I have a 1-version Cropper, 1-version Detector, and 2-version Reader.</p><pre><code>Cropper/
  1/

Detector/
  1/

Reader/
  1/
  2/
    saved_model.pb
    variables/
      variables.index
</code></pre><p>And you can load all of these models into Tensorflow Serving at once (YES, multiple models serving) by specifying a model base path config (for example, <code>serving.config</code>):</p><pre><code>model_config_list: {
  config: {
      name: &quot;cropper&quot;,
      base_path: &quot;/absolute/path/to/cropper&quot;,
      model_platform: &quot;tensorflow&quot;
  },
  config: {
      name: &quot;detector&quot;,
      base_path: &quot;/absolute/path/to/detector&quot;,
      model_platform: &quot;tensorflow&quot;
  },
  config: {
      name: &quot;reader&quot;,
      base_path: &quot;/absolute/path/to/reader&quot;,
      model_platform: &quot;tensorflow&quot;
  }
}
</code></pre><p>and boot up Tensorflow Serving with appropriate flag:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>tensorflow_model_server --model_config_file<span style=color:#f92672>=</span>/absolute/path/to/serving.config
</code></pre></div><p>The trouble way is you have to export model with trained weights to SavedModel format first.</p><p><img loading=lazy src=/ocr/Export.png alt=export></p><p>A sample script to export I provide here. The trick is you make use of <a href=https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/training/saver.py>Tensorflow Saver</a>.</p><blockquote><p>Specifically, SavedModel wraps a TensorFlow Saver. The Saver is primarily used to generate the variable checkpoints. <a href=https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model#background>source</a></p></blockquote><p>And the second important thing is you understand what input and output node your model is having.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># ...</span>

<span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>Session() <span style=color:#66d9ef>as</span> sess:
    tf<span style=color:#f92672>.</span>train<span style=color:#f92672>.</span>Saver()<span style=color:#f92672>.</span>restore(sess, FLAGS<span style=color:#f92672>.</span>checkpoint)

    <span style=color:#75715e># images_placeholder as input node</span>
    inputs <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;input&#39;</span>: tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>build_tensor_info(INPUT_PLACEHOLDER)}

    <span style=color:#75715e># get output node through node name</span>
    out_classes <span style=color:#f92672>=</span> sess<span style=color:#f92672>.</span>graph<span style=color:#f92672>.</span>get_tensor_by_name(<span style=color:#e6db74>&#39;YOUR_OUTPUT_NODE_NAME&#39;</span>)
    outputs <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;output&#39;</span>:  tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>build_tensor_info(out_classes)}

    signature <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>signature_def_utils<span style=color:#f92672>.</span>build_signature_def(
        inputs<span style=color:#f92672>=</span>inputs,
        outputs<span style=color:#f92672>=</span>outputs,
        method_name<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>signature_constants<span style=color:#f92672>.</span>PREDICT_METHOD_NAME)

    legacy_init_op <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>group(tf<span style=color:#f92672>.</span>tables_initializer(), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;legacy_init_op&#39;</span>)

    <span style=color:#75715e># Save out the SavedModel</span>
    builder <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>builder<span style=color:#f92672>.</span>SavedModelBuilder(FLAGS<span style=color:#f92672>.</span>saved_dir)
    builder<span style=color:#f92672>.</span>add_meta_graph_and_variables(
        sess, [tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>tag_constants<span style=color:#f92672>.</span>SERVING],
        signature_def_map<span style=color:#f92672>=</span>{
            tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>signature_constants<span style=color:#f92672>.</span>
            DEFAULT_SERVING_SIGNATURE_DEF_KEY:
            signature
        },
        legacy_init_op<span style=color:#f92672>=</span>legacy_init_op)
    builder<span style=color:#f92672>.</span>save()
</code></pre></div><h2 id=backend-service>Backend service<a hidden class=anchor aria-hidden=true href=#backend-service>#</a></h2><p>Now is the full look of our backend service:</p><p><img loading=lazy src=/ocr/Backend.png alt=backend></p><p>There are 2 small services (components) we mainly take care of: <code>api</code> and <code>serving</code>.</p><p><code>serving</code> is Tensorflow Serving as we mentioned, and <code>api</code> (which provides RESTful API for clients) connects with <code>serving</code> through <a href=https://grpc.io/>grpc</a>.
Luckily (also), we don&rsquo;t have to take care much of Protocol Buffers logics and just make use of <a href=https://pypi.org/project/tensorflow-serving-api/>tensorflow-serving-api</a> library.
A sample use I provide:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf
<span style=color:#f92672>from</span> tensorflow_serving.apis <span style=color:#f92672>import</span> predict_pb2
<span style=color:#f92672>from</span> tensorflow_serving.apis.prediction_service_pb2_grpc <span style=color:#f92672>import</span> PredictionServiceStub

<span style=color:#f92672>import</span> grpc
<span style=color:#f92672>import</span> cv2


<span style=color:#75715e># Your config</span>
SERVICE_HOST <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;localhost:8500&#39;</span> <span style=color:#75715e># sample</span>
MODEL_NAME <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;reader&#39;</span>
INPUT_SIGNATURE_KEY <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;input&#39;</span>
OUTPUT_SIGNATURE_KEY <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;output&#39;</span>

<span style=color:#75715e>#######</span>

<span style=color:#75715e># Connect to server</span>
channel <span style=color:#f92672>=</span> grpc<span style=color:#f92672>.</span>insecure_channel(SERVICE_HOST)
stub <span style=color:#f92672>=</span> PredictionServiceStub(channel)

<span style=color:#75715e># Prepare request object</span>
request <span style=color:#f92672>=</span> predict_pb2<span style=color:#f92672>.</span>PredictRequest()
request<span style=color:#f92672>.</span>model_spec<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> MODEL_NAME
request<span style=color:#f92672>.</span>model_spec<span style=color:#f92672>.</span>signature_name <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>saved_model<span style=color:#f92672>.</span>signature_constants<span style=color:#f92672>.</span>DEFAULT_SERVING_SIGNATURE_DEF_KEY

<span style=color:#75715e># Copy image into request&#39;s content</span>
img <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>imread(<span style=color:#e6db74>&#39;/path/to/image&#39;</span>)
input_tensor <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(img, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#75715e># we do batch inferencing, so input is a 4-D tensor</span>
request<span style=color:#f92672>.</span>inputs[INPUT_SIGNATURE_KEY]<span style=color:#f92672>.</span>CopyFrom(
    tf<span style=color:#f92672>.</span>contrib<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>make_tensor_proto(input_tensor))

<span style=color:#75715e># Do inference</span>
result <span style=color:#f92672>=</span> stub<span style=color:#f92672>.</span>Predict<span style=color:#f92672>.</span>future(request, <span style=color:#ae81ff>5</span>) <span style=color:#75715e># 5s timeout</span>

<span style=color:#75715e># Get output depends on our input/output signature, and their types</span>
<span style=color:#75715e># for example, our output signature key is &#39;output&#39; and has string value</span>
word <span style=color:#f92672>=</span> result<span style=color:#f92672>.</span>result()<span style=color:#f92672>.</span>outputs[OUTPUT_SIGNATURE_KEY]<span style=color:#f92672>.</span>string_val[<span style=color:#ae81ff>0</span>] <span style=color:#75715e># we have batch result, so just take first index</span>
</code></pre></div><h1 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h1><p>Deployment is an important part, but usually a myth among Deep Learning articles.
Tensorflow Serving <a href=https://www.tensorflow.org/serving/serving_kubernetes>guide</a> demos deploy in Kubernetes with built container images.
But we found it over-complicated and decided to use <a href=https://docs.docker.com/compose/>Docker Compose</a> for simply running two pre-built images <code>api</code> and <code>serving</code>.</p><p>We have two types of deploy machines, only CPU platform, and with GPU platform.</p><p>Only CPU machine goes with common <code>docker-compose.yml</code>, with <code>Dockerfile</code> for <code>api</code>, and <code>Dockerfile.serving</code> for serving which is based from <a href=https://hub.docker.com/r/tensorflow/serving/><code>tensorflow/serving:latest-devel</code></a> image.</p><p><img loading=lazy src=/ocr/cpu.png alt=cpu></p><p>On the other hand, GPU machine goes with custom <code>nvidia-docker-compose.yml</code> (which requires <a href=https://github.com/NVIDIA/nvidia-docker>nvidia-docker</a>), same <code>Dockerfile</code> for <code>api,</code> and <code>Dockerfile.serving.gpu</code> for <code>serving</code> (which is based from <code>tensorflow:latest-devel-gpu</code>).</p><p><img loading=lazy src=/ocr/gpu.png alt=gpu></p><p>One main note for running Docker Compose with GPU is inside <code>nvidia-docker-compose.yml</code> you have to specify <code>runtime: nvidia</code>.
A troublesome problem is Docker Compose default uses all GPUs so you have to set <code>NVIDIA_VISIBLE_DEVICES</code> environment variable to your dedicated running GPUs.
A trick I use for consistent use between Tensorflow (<code>CUDA_VISIBLE_DEVICES</code>) and Docker Compose is:</p><pre><code># nvidia-docker-compose.yml

version: '2.3'
services:
  serving:
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-all}
    [...]
  [...]
</code></pre><p>By doing that, if you set <code>CUDA_VISIBLE_DEVICES</code> you use what GPUs you want to use. Otherwise it uses all GPUs by default.</p><h1 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h1><p>I shared you our research, architecture, and deployment of a complete OCR system.
It uses the state-of-the-art deep learning OCR model (Attention OCR), scalable with Tensorflow Serving, and ready for production deployment with the help of Docker Compose.</p><p>By using Tensorflow we have an entire ecosystem backed by Google, a typical benefit is Tensorflow Serving (which belongs to <a href=https://www.tensorflow.org/tfx/>TFX</a>).
A huge support is from the community for model implementation, too.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://hiepph.xyz>hiepph</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>